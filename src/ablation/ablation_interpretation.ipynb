{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8631ee0fb342920d",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-01-25T10:30:30.397143Z",
     "start_time": "2024-01-25T10:30:25.980492Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'component': 'mha',\n",
      " 'dataset': 'rephrased',\n",
      " 'effect': 'absolute',\n",
      " 'encoder': 'vision',\n",
      " 'metric': 'difference',\n",
      " 'negation': 'foil',\n",
      " 'segment': 'correct'}\n",
      "Input examples: 81\n"
     ]
    }
   ],
   "source": [
    "from src.utils.data_utils import load_dataset, generate_clip_input\n",
    "from src.utils.model_utils import load_model\n",
    "from src.utils.ablation_utils import create_img_storage_hook_fn, create_ablation_hook_fn, store_img_ablation_results\n",
    "\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Experiment configuration\n",
    "CONFIG = {\n",
    "    'encoder': 'vision',    # vision/text\n",
    "    'component': 'mha',     # mha/mlp\n",
    "    'dataset': 'rephrased', # standard/rephrased\n",
    "    'negation': 'foil',  # foil/caption\n",
    "    'metric': 'difference', # absolute/difference\n",
    "    'segment': 'correct',  # correct/ambiguous/incorrect\n",
    "    'effect': 'absolute'    # absolute/relative\n",
    "}\n",
    "\n",
    "# Load dataset and model\n",
    "dataset = load_dataset(**CONFIG)\n",
    "pprint(CONFIG)\n",
    "print(f'Input examples: {len(dataset)}')\n",
    "\n",
    "model, processor = load_model()\n",
    "ablated_model, _ = load_model()\n",
    "\n",
    "# Obtain relevant model variables\n",
    "n_layers = model.vision_model.config.num_hidden_layers\n",
    "hidden_size = model.vision_model.config.hidden_size\n",
    "patch_size = model.vision_model.config.patch_size\n",
    "image_size = model.vision_model.config.image_size\n",
    "tokens = (image_size // patch_size) ** 2 + 1 # + cls token\n",
    "\n",
    "activations = defaultdict(lambda: torch.zeros((tokens, hidden_size)))\n",
    "\n",
    "all_hooks = []\n",
    "for layer_idx, layer in enumerate(model.vision_model.encoder.layers):\n",
    "    storage_hook_fn = create_img_storage_hook_fn(layer_idx, activations)\n",
    "    storage_hook = layer.self_attn.register_forward_hook(storage_hook_fn)\n",
    "    all_hooks.append(storage_hook)\n",
    "\n",
    "total_score_differences = {'sum': torch.zeros(n_layers), 'count': 0}\n",
    "before_after_dict = defaultdict(lambda: list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "total_caption_changes = {'sum': torch.zeros(n_layers), 'count': 0}\n",
    "total_foil_changes = {'sum': torch.zeros(n_layers), 'count': 0}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-25T10:30:30.398722Z",
     "start_time": "2024-01-25T10:30:30.396557Z"
    }
   },
   "id": "81b70f851b52983c"
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 81/81 [01:10<00:00,  1.14it/s]\n"
     ]
    }
   ],
   "source": [
    "for data in tqdm(dataset.values()):\n",
    "    inputs = generate_clip_input(data, processor)\n",
    "    # Get pixel values and calculate average\n",
    "    img = inputs.pixel_values\n",
    "    avg_img = img.mean(dim=(-2, -1), keepdim=True).expand_as(img)\n",
    "    # Create new input objects with averaged pixel values\n",
    "    new_inputs = inputs.copy()\n",
    "    new_inputs['pixel_values'] = avg_img\n",
    "    # Forward pass to record activations with averaged image\n",
    "    model(**new_inputs)\n",
    "\n",
    "    # Obtain score of instance without any ablation\n",
    "    if CONFIG['metric'] == 'absolute':\n",
    "        score = data['logit_caption']\n",
    "    if CONFIG['metric'] == 'difference':\n",
    "        score = data['score']\n",
    "    # Set up empty tensors to store ablation results for this instance\n",
    "    score_differences = torch.zeros(n_layers)\n",
    "    caption_changes = torch.zeros(n_layers)\n",
    "    foil_changes = torch.zeros(n_layers)\n",
    "\n",
    "    # Loop over layers to do ablation\n",
    "    for l in range(n_layers):\n",
    "        # Create hook function for this layer\n",
    "        hook_fn = create_ablation_hook_fn(layer=l, mean_activations=activations)\n",
    "        # Register hook in this layer of the model\n",
    "        ablation_hook = ablated_model.vision_model.encoder.layers[l].self_attn.register_forward_hook(hook_fn)\n",
    "        # Run forward pass to get output with ablation\n",
    "        output = ablated_model(**inputs)\n",
    "        if CONFIG['metric'] == 'absolute':\n",
    "            ablated_score = output.logits_per_text[0].item()\n",
    "        if CONFIG['metric'] == 'difference':\n",
    "            ablated_score = (output.logits_per_text[0] - output.logits_per_text[1]).item()\n",
    "        # Save score difference (normal - ablated) for this layer\n",
    "        if CONFIG['effect'] == 'absolute':\n",
    "            score_differences[l] = score - ablated_score\n",
    "        if CONFIG['effect'] == 'relative':\n",
    "            score_differences[l] = ablated_score / score\n",
    "        # Save original and ablated score to dict\n",
    "        before_after_dict[l].append((score, ablated_score))\n",
    "        \n",
    "        caption_change = output.logits_per_text.squeeze()[0].item() / data['logit_caption']\n",
    "        caption_changes[l] = caption_change\n",
    "        foil_change = output.logits_per_text.squeeze()[1].item() / data['logit_foil']\n",
    "        foil_changes[l] = foil_change\n",
    "        \n",
    "        # Remove hook\n",
    "        ablation_hook.remove()\n",
    "        # break\n",
    "    # break\n",
    "    # Save results from this example to total results dict\n",
    "    total_score_differences['sum'] += score_differences\n",
    "    total_score_differences['count'] += 1\n",
    "    total_caption_changes['sum'] += caption_changes\n",
    "    total_caption_changes['count'] += 1\n",
    "    total_foil_changes['sum'] += foil_changes\n",
    "    total_foil_changes['count'] += 1\n",
    "\n",
    "# before_after = np.array([(layer, orig, ablat) for layer, scores in before_after_dict.items() for orig, ablat in scores])\n",
    "# mean_ablation_effect = (total_score_differences['sum'] / total_score_differences['count']).numpy()\n",
    "\n",
    "# Remove remaining hooks\n",
    "for hook in all_hooks:\n",
    "    hook.remove()\n",
    "\n",
    "# Save results\n",
    "# store_img_ablation_results(mean_ablation_effect, before_after, CONFIG)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-25T10:31:45.106035Z",
     "start_time": "2024-01-25T10:30:34.294003Z"
    }
   },
   "id": "initial_id"
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "data": {
      "text/plain": "array([0.99357116, 1.0002027 , 1.0005752 , 1.0066627 , 1.0103865 ,\n       0.994613  , 0.9937228 , 0.9924664 , 0.97932357, 1.0231844 ,\n       1.0202563 , 0.9660136 ], dtype=float32)"
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(total_caption_changes['sum'] / total_caption_changes['count']).numpy()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-25T10:36:10.272207Z",
     "start_time": "2024-01-25T10:36:10.257250Z"
    }
   },
   "id": "57b76211a0e0d3c8"
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "data": {
      "text/plain": "array([1.0399709 , 1.0089185 , 1.0045177 , 1.0214515 , 1.023455  ,\n       1.0204171 , 1.0043926 , 1.0076008 , 0.9931397 , 1.0382454 ,\n       1.0302467 , 0.98561096], dtype=float32)"
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(total_foil_changes['sum'] / total_foil_changes['count']).numpy()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-25T10:36:12.127913Z",
     "start_time": "2024-01-25T10:36:12.122988Z"
    }
   },
   "id": "9647e5487029cc80"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# MLP"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "44d7148b1e263bbc"
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'component': 'mlp',\n",
      " 'dataset': 'rephrased',\n",
      " 'effect': 'absolute',\n",
      " 'encoder': 'vision',\n",
      " 'metric': 'difference',\n",
      " 'negation': 'foil',\n",
      " 'segment': 'correct'}\n",
      "Input examples: 81\n"
     ]
    }
   ],
   "source": [
    "from src.utils.data_utils import load_dataset, generate_clip_input\n",
    "from src.utils.model_utils import load_model\n",
    "from src.utils.ablation_utils import create_mlp_storage_hook_fn, create_mlp_ablation_hook_fn, store_img_ablation_results\n",
    "\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Experiment configuration\n",
    "CONFIG = {\n",
    "    'encoder': 'vision',    # vision/text\n",
    "    'component': 'mlp',     # mha/mlp\n",
    "    'dataset': 'rephrased', # standard/rephrased\n",
    "    'negation': 'foil',  # foil/caption\n",
    "    'metric': 'difference', # absolute/difference\n",
    "    'segment': 'correct',  # correct/ambiguous/incorrect\n",
    "    'effect': 'absolute'    # absolute/relative\n",
    "}\n",
    "\n",
    "# Load dataset and model\n",
    "dataset = load_dataset(**CONFIG)\n",
    "pprint(CONFIG)\n",
    "print(f'Input examples: {len(dataset)}')\n",
    "\n",
    "model, processor = load_model()\n",
    "ablated_model, _ = load_model()\n",
    "\n",
    "# Obtain relevant model variables\n",
    "n_layers = model.vision_model.config.num_hidden_layers\n",
    "hidden_size = model.vision_model.config.hidden_size\n",
    "patch_size = model.vision_model.config.patch_size\n",
    "image_size = model.vision_model.config.image_size\n",
    "tokens = (image_size // patch_size) ** 2 + 1 # + cls token\n",
    "\n",
    "activations = defaultdict(lambda: torch.zeros((tokens, hidden_size)))\n",
    "\n",
    "all_hooks = []\n",
    "for layer_idx, layer in enumerate(model.vision_model.encoder.layers):\n",
    "    storage_hook_fn = create_mlp_storage_hook_fn(layer_idx, activations)\n",
    "    storage_hook = layer.mlp.register_forward_hook(storage_hook_fn)\n",
    "    all_hooks.append(storage_hook)\n",
    "\n",
    "total_score_differences = {'sum': torch.zeros(n_layers), 'count': 0}\n",
    "before_after_dict = defaultdict(lambda: list())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-25T10:23:18.560980Z",
     "start_time": "2024-01-25T10:23:14.082352Z"
    }
   },
   "id": "606ebe32defa7d87"
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "total_caption_changes = {'sum': torch.zeros(n_layers), 'count': 0}\n",
    "total_foil_changes = {'sum': torch.zeros(n_layers), 'count': 0}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-25T10:23:18.564281Z",
     "start_time": "2024-01-25T10:23:18.560789Z"
    }
   },
   "id": "e2b9352b1e17daf6"
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 81/81 [01:07<00:00,  1.20it/s]\n"
     ]
    }
   ],
   "source": [
    "for data in tqdm(dataset.values()):\n",
    "    inputs = generate_clip_input(data, processor)\n",
    "    # Get pixel values and calculate average\n",
    "    img = inputs.pixel_values\n",
    "    avg_img = img.mean(dim=(-2, -1), keepdim=True).expand_as(img)\n",
    "    # Create new input objects with averaged pixel values\n",
    "    new_inputs = inputs.copy()\n",
    "    new_inputs['pixel_values'] = avg_img\n",
    "    # Forward pass to record activations with averaged image\n",
    "    model(**new_inputs)\n",
    "\n",
    "    # Obtain score of instance without any ablation\n",
    "    if CONFIG['metric'] == 'absolute':\n",
    "        score = data['logit_caption']\n",
    "    if CONFIG['metric'] == 'difference':\n",
    "        score = data['score']\n",
    "    # Set up empty tensor to store ablation results for this instance\n",
    "    score_differences = torch.zeros(n_layers)\n",
    "    caption_changes = torch.zeros(n_layers)\n",
    "    foil_changes = torch.zeros(n_layers)\n",
    "\n",
    "    # Loop over layers to do ablation\n",
    "    for l in range(n_layers):\n",
    "        # Create hook function for this layer\n",
    "        hook_fn = create_mlp_ablation_hook_fn(layer=l, mean_activations=activations)\n",
    "        # Register hook in this layer of the model\n",
    "        ablation_hook = ablated_model.vision_model.encoder.layers[l].mlp.register_forward_hook(hook_fn)\n",
    "        # Run forward pass to get output with ablation\n",
    "        output = ablated_model(**inputs)\n",
    "        if CONFIG['metric'] == 'absolute':\n",
    "            ablated_score = output.logits_per_text[0].item()\n",
    "        if CONFIG['metric'] == 'difference':\n",
    "            ablated_score = (output.logits_per_text[0] - output.logits_per_text[1]).item()\n",
    "        # Save score difference (normal - ablated) for this layer\n",
    "        if CONFIG['effect'] == 'absolute':\n",
    "            score_differences[l] = score - ablated_score\n",
    "        if CONFIG['effect'] == 'relative':\n",
    "            score_differences[l] = ablated_score / score\n",
    "        # Save original and ablated score to dict\n",
    "        before_after_dict[l].append((score, ablated_score))\n",
    "        \n",
    "        caption_change = output.logits_per_text.squeeze()[0].item() / data['logit_caption']\n",
    "        caption_changes[l] = caption_change\n",
    "        foil_change = output.logits_per_text.squeeze()[1].item() / data['logit_foil']\n",
    "        foil_changes[l] = foil_change\n",
    "        \n",
    "        # Remove hook\n",
    "        ablation_hook.remove()\n",
    "\n",
    "    # Save results from this example to total results dict\n",
    "    total_score_differences['sum'] += score_differences\n",
    "    total_score_differences['count'] += 1\n",
    "    total_caption_changes['sum'] += caption_changes\n",
    "    total_caption_changes['count'] += 1\n",
    "    total_foil_changes['sum'] += foil_changes\n",
    "    total_foil_changes['count'] += 1"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-25T10:24:25.946176Z",
     "start_time": "2024-01-25T10:23:18.567233Z"
    }
   },
   "id": "796a7c9c92c1dc0e"
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "data": {
      "text/plain": "array([0.94712895, 0.9166725 , 0.97669154, 1.0067167 , 0.99166006,\n       0.98826486, 0.9967601 , 0.99160933, 0.99351984, 0.9912236 ,\n       1.0121447 , 1.0395426 ], dtype=float32)"
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(total_caption_changes['sum'] / total_caption_changes['count']).numpy()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-25T10:29:45.637309Z",
     "start_time": "2024-01-25T10:29:45.627696Z"
    }
   },
   "id": "d52361749239077d"
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "data": {
      "text/plain": "array([0.99753875, 0.9700377 , 1.0202836 , 1.0450764 , 1.0166669 ,\n       1.0156177 , 1.0074837 , 1.0018388 , 1.0105493 , 1.0108528 ,\n       1.029155  , 1.0385493 ], dtype=float32)"
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(total_foil_changes['sum'] / total_foil_changes['count']).numpy()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-25T10:29:49.085583Z",
     "start_time": "2024-01-25T10:29:49.060630Z"
    }
   },
   "id": "3a2ba4f0d5b647ec"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "fbd4c790e77b6b96"
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "clip-negation",
   "language": "python",
   "display_name": "Python (clip-negation)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
